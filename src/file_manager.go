package main

import (
	"bufio"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"time"

	"github.com/google/uuid"
)

type FileManager struct {
	dbManager *DBManager
}

func NewFileManager(db *DBManager) *FileManager {
	return &FileManager{dbManager: db}
}

func (fm *FileManager) generateTaskID(filename string) string {
	return fmt.Sprintf("task_%s_%s", time.Now().Format("20060102_150405"), uuid.New().String()[:8])
}

func (fm *FileManager) generateChunkID(taskID string, chunkIndex int, retry int) string {
	if retry > 0 {
		return fmt.Sprintf("%s_retry%d_chunk_%d", taskID, retry, chunkIndex)
	}
	return fmt.Sprintf("%s_chunk_%d", taskID, chunkIndex)
}

func (fm *FileManager) writeChunk(taskID string, chunkIndex int, originalFilename string,
	chunkDir string, currentChunkLines []string, fileInfo *FileInfo, retry int) error {
	chunkID := fm.generateChunkID(taskID, chunkIndex, retry)
	var chunkFilename string
	if retry > 0 {
		chunkFilename = fmt.Sprintf("retry%d_part%d.%s", retry, chunkIndex, originalFilename)
	} else {
		chunkFilename = fmt.Sprintf("part%d.%s", chunkIndex, originalFilename)
	}
	chunkPath := filepath.Join(chunkDir, chunkFilename)
	chunkData := strings.Join(currentChunkLines, "\n")
	os.WriteFile(chunkPath, []byte(chunkData), 0644)
	chunk := &FileChunk{
		ChunkID:    chunkID,
		TaskID:     taskID,
		ChunkIndex: chunkIndex,
		ChunkPath:  chunkPath,
		ChunkSize:  len([]byte(chunkData)),
		Status:     ChunkStatusPending,
		Retry:      retry,
	}
	return fm.dbManager.AddChunk(chunk)
}

// SplitFile: åŒ…å«å…¨é‡é”™è¯¯æ‹¦æˆªã€æµ‹è¯•è¡Œæ•°æˆªæ–­ä»¥åŠé¢„ä¼°è€—æ—¶åŠŸèƒ½
func (fm *FileManager) SplitFile(filePath string, originalFilename string, linesPerChunk int, taskID string) (*FileInfo, error) {
	file, err := os.Open(filePath)
	if err != nil {
		return nil, fmt.Errorf("æ–‡ä»¶ä¸å­˜åœ¨")
	}
	defer file.Close()

	if taskID == "" {
		taskID = fm.generateTaskID(originalFilename)
	}
	exists, _ := fm.dbManager.CheckTaskIDExists(taskID)
	if exists {
		return nil, fmt.Errorf("task_id [%s] å·²å­˜åœ¨", taskID)
	}

	stat, _ := file.Stat()
	fileInfoObj := &FileInfo{
		TaskID:           taskID,
		OriginalFilename: originalFilename,
		FilePath:         filePath,
		FileSize:         stat.Size(),
		CreatedTime:      time.Now().Format(time.RFC3339),
		UpdatedTime:      time.Now().Format(time.RFC3339),
		Status:           FileStatusSplitting,
		Chunks:           []*FileChunk{},
		MaxRetry:         MAX_RETRY_COUNT,
	}
	fm.dbManager.CreateFile(fileInfoObj)

	chunkDir := filepath.Join(CHUNK_DIR, taskID)
	os.MkdirAll(chunkDir, 0755)

	// --- å‡†å¤‡é”™è¯¯è®°å½•æ–‡ä»¶ ---
	validationDir := filepath.Join(BATCH_RESULT_DIR, taskID, "validation")
	os.MkdirAll(validationDir, 0755)
	errorFilePath := filepath.Join(validationDir, "format_errors.jsonl")

	errorFile, err := os.Create(errorFilePath)
	var errorWriter *bufio.Writer
	if err == nil {
		errorWriter = bufio.NewWriter(errorFile)
		defer errorFile.Close()
	}

	scanner := bufio.NewScanner(file)
	buf := make([]byte, 0, 64*1024)
	scanner.Buffer(buf, 10*1024*1024)

	// åˆå§‹åŒ–å¾ªç¯å˜é‡
	chunkIndex := 0
	totalLines := 0
	scanLineIndex := 0
	errorCount := 0
	currentChunkLines := []string{}

	fmt.Printf("å¼€å§‹å…¨é‡æ ¼å¼æ‰«æ: %s ...\n", originalFilename)

	for scanner.Scan() {
		scanLineIndex++
		rawLine := scanner.Text()
		line := strings.TrimSpace(rawLine)

		if line == "" {
			continue
		}

		var originJSON map[string]interface{}
		var parseError string = ""

		// 1. è¯­æ³•ä¸ä¸šåŠ¡å­—æ®µæ£€æŸ¥
		if err := json.Unmarshal([]byte(line), &originJSON); err != nil {
			parseError = "Invalid JSON syntax"
		} else {
			if _, ok := originJSON[ModelConf.MessagesKey].([]interface{}); !ok {
				parseError = fmt.Sprintf("Missing or invalid field: %s", ModelConf.MessagesKey)
			}
		}

		// 2. å‘ç°é”™è¯¯ï¼šè®°å½•å¹¶ç»Ÿè®¡
		if parseError != "" {
			errorCount++
			if errorWriter != nil {
				errRecord := map[string]interface{}{
					"line_no": scanLineIndex,
					"error":   parseError,
					"content": line,
				}
				errBytes, _ := json.Marshal(errRecord)
				errorWriter.WriteString(string(errBytes) + "\n")
			}
			if errorCount <= 5 {
				fmt.Printf("âš ï¸  [ç¬¬ %d è¡Œ] æ ¼å¼é”™è¯¯: %s\n", scanLineIndex, parseError)
			}
			continue
		}

		// 3. å¤„ç†æœ‰æ•ˆæ•°æ®
		messages, _ := originJSON[ModelConf.MessagesKey].([]interface{})
		body := map[string]interface{}{
			"model":      ModelConf.Domain,
			"messages":   messages,
			"max_tokens": ModelConf.MaxTokens,
		}
		
		newline := map[string]interface{}{
			"custom_id": fmt.Sprintf("%d", totalLines),
			"method":    "POST",
			"url":       "/v1/chat/completions",
			"body":      body,
		}
		newlineJSON, _ := json.Marshal(newline)
		currentChunkLines = append(currentChunkLines, string(newlineJSON))
		totalLines++

		// 4. æµ‹è¯•è¡Œæ•°æˆªæ–­é€»è¾‘ (ä»…åœ¨æœ‰æ•ˆæ•°æ®å¤„ç†ååˆ¤æ–­)
		if TEST_LINES > 0 && totalLines >= TEST_LINES {
			fmt.Printf("âš ï¸  [æµ‹è¯•æ¨¡å¼] å·²è¾¾åˆ°æµ‹è¯•è¡Œæ•°é™åˆ¶: %d è¡Œï¼Œåœæ­¢åç»­æ‰«æã€‚\n", TEST_LINES)
			break
		}

		// 5. å†™å…¥åˆ†å—
		if len(currentChunkLines) >= linesPerChunk {
			fm.writeChunk(taskID, chunkIndex, originalFilename, chunkDir, currentChunkLines, fileInfoObj, fileInfoObj.Retry)
			chunkIndex++
			currentChunkLines = []string{}
		}
	}

	if errorWriter != nil {
		errorWriter.Flush()
	}

	// 6. æ‰«æå®Œæˆåç»Ÿä¸€æ‹¦æˆªé”™è¯¯
	if errorCount > 0 {
		absPath, _ := filepath.Abs(errorFilePath)
		fmt.Println("----------------------------------------------------------------")
		fmt.Printf("âŒ æ‰«æå®Œæˆï¼Œå‘ç°æ•°æ®å¼‚å¸¸ï¼ä»»åŠ¡å·²å¼ºåˆ¶æ‹¦æˆªã€‚\n")
		fmt.Printf("   - æ‰«ææ€»è¡Œæ•°: %d\n", scanLineIndex)
		fmt.Printf("   - é”™è¯¯è¡Œæ€»æ•°: %d\n", errorCount)
		fmt.Printf("ğŸ“‚ å®Œæ•´é”™è¯¯æ¸…å•è¯·æŸ¥çœ‹:\n   %s\n", absPath)
		fmt.Println("----------------------------------------------------------------")
		os.RemoveAll(chunkDir)
		return nil, fmt.Errorf("æ•°æ®æ–‡ä»¶ä¸­å­˜åœ¨ %d å¤„æ ¼å¼é”™è¯¯ï¼Œè¯·æ ¹æ®é”™è¯¯æ¸…å•ä¿®æ”¹åé‡è¯•", errorCount)
	}

	// 7. å¤„ç†å‰©ä½™æ•°æ®
	if len(currentChunkLines) > 0 {
		fm.writeChunk(taskID, chunkIndex, originalFilename, chunkDir, currentChunkLines, fileInfoObj, fileInfoObj.Retry)
		chunkIndex++
	}

	// 8. ç»Ÿè®¡æ—¥å¿—
	taskLogger := NewTaskLogger(taskID)
	taskLogger.LogTimeEstimate(totalLines)

	fmt.Printf("ğŸ“Š æ–‡ä»¶æ‰«æç»Ÿè®¡ | ä»»åŠ¡ID: %s\n", taskID)
	fmt.Printf("   - æ‰«ææ€»è¡Œæ•°: %d\n", scanLineIndex)
	fmt.Printf("   - âœ… æœ‰æ•ˆæ•°æ®: %d\n", totalLines)
	fmt.Printf("   - âŒ é”™è¯¯æ•°æ®: %d\n", errorCount)
	fmt.Println("----------------------------------------------------------------")

	fileInfoObj.TotalChunks = chunkIndex
	fileInfoObj.TotalLines = totalLines
	fm.dbManager.UpdateFileStatus(taskID, FileStatusSplitCompleted, nil)
	fm.dbManager.UpdateFileTotalChunks(taskID, chunkIndex)
	fm.dbManager.UpdateFileTotalLines(taskID, totalLines)
	
	return fileInfoObj, nil
}

func (fm *FileManager) SaveFile(taskID string, chunkID string, fileContent string, isError bool) error {
	chunk, _ := fm.dbManager.GetChunk(chunkID)
	if chunk == nil {
		return fmt.Errorf("chunk not found")
	}
	path := filepath.Join(BATCH_RESULT_DIR, taskID, "output")
	if isError {
		path = filepath.Join(BATCH_RESULT_DIR, taskID, "error")
	}
	os.MkdirAll(path, 0755)
	return os.WriteFile(filepath.Join(path, fmt.Sprintf("retry%d_%s.jsonl", chunk.Retry, chunkID)), []byte(fileContent), 0644)
}

func (fm *FileManager) isFatalError(statusCode int, errCode string) (bool, string) {
	if statusCode >= 400 && statusCode < 500 {
		if statusCode == 429 {
			return false, "Rate Limit (Retryable)"
		}
		return true, fmt.Sprintf("HTTP %d (Fatal)", statusCode)
	}
	fatalCodes := map[string]bool{
		"context_length_exceeded": true, "invalid_request_error": true,
		"invalid_api_key": true, "unknown_url": true,
		"model_not_found": true, "10003": true, "invalid_role": true,
		"INVALID_PAYLOAD": true,
	}
	if fatalCodes[errCode] { return true, fmt.Sprintf("Error Code: %s", errCode) }
	return false, "Retryable Error"
}

func (fm *FileManager) MergeBatchResults(taskID string, retry int) (map[string]interface{}, error) {
	fileInfo, err := fm.dbManager.GetFile(taskID)
	if err != nil || fileInfo == nil {
		return nil, fmt.Errorf("ä»»åŠ¡ä¸å­˜åœ¨")
	}

	var chunks []*FileChunk
	for _, c := range fileInfo.Chunks {
		if c.Retry == retry {
			chunks = append(chunks, c)
		}
	}
	sort.Slice(chunks, func(i, j int) bool { return chunks[i].ChunkIndex < chunks[j].ChunkIndex })

	allOutput := []string{}
	succeededIDs := make(map[string]bool)
	fatalErrorIDs := make(map[string]bool)

	type FailureRecord struct {
		CustomID string; StatusCode int; ErrorCode string; ErrorMsg string; IsFatal bool
	}
	failureReport := []FailureRecord{}

	mergedDir := filepath.Join(MERGED_DIR, taskID)
	os.MkdirAll(mergedDir, 0755)

	for _, chunk := range chunks {
		outputFile := filepath.Join(BATCH_RESULT_DIR, taskID, "output", fmt.Sprintf("retry%d_%s.jsonl", chunk.Retry, chunk.ChunkID))
		if f, err := os.Open(outputFile); err == nil {
			scanner := bufio.NewScanner(f)
			for scanner.Scan() {
				line := scanner.Text()
				allOutput = append(allOutput, line)
				var r map[string]interface{}
				if json.Unmarshal([]byte(line), &r) == nil {
					if cid, ok := r["custom_id"].(string); ok { succeededIDs[cid] = true }
				}
			}
			f.Close()
		}

		errorFile := filepath.Join(BATCH_RESULT_DIR, taskID, "error", fmt.Sprintf("retry%d_%s.jsonl", chunk.Retry, chunk.ChunkID))
		if f, err := os.Open(errorFile); err == nil {
			scanner := bufio.NewScanner(f)
			for scanner.Scan() {
				line := scanner.Text()
				var r struct {
					CustomID string `json:"custom_id"`
					Response struct {
						StatusCode int `json:"status_code"`
						Body struct {
							Code interface{} `json:"code"`; Message string `json:"message"`
							Error struct { Code string `json:"code"`; Message string `json:"message"` } `json:"error"`
						} `json:"body"`
					} `json:"response"`
				}
				if json.Unmarshal([]byte(line), &r) == nil && r.CustomID != "" {
					var bizCode int = 0
					if r.Response.Body.Code != nil {
						if v, ok := r.Response.Body.Code.(float64); ok { bizCode = int(v) }
						if v, ok := r.Response.Body.Code.(int); ok { bizCode = v }
					}
					if r.Response.StatusCode > 0 && r.Response.StatusCode < 300 && bizCode == 0 { continue }
					
					fCode := r.Response.Body.Error.Code; fMsg := r.Response.Body.Error.Message
					if fCode == "" && bizCode != 0 { fCode = fmt.Sprintf("%d", bizCode); fMsg = r.Response.Body.Message }
					
					if r.Response.StatusCode == 0 {
						fCode = "INVALID_PAYLOAD"; fMsg = "APIç›´æ¥æ‹’æ”¶è¯·æ±‚"; r.Response.StatusCode = 400
					}

					isFatal, _ := fm.isFatalError(r.Response.StatusCode, fCode)
					if isFatal { fatalErrorIDs[r.CustomID] = true }
					failureReport = append(failureReport, FailureRecord{r.CustomID, r.Response.StatusCode, fCode, fMsg, isFatal})
				}
			}
			f.Close()
		}
	}

	needRetryLines := []string{}
	for _, chunk := range chunks {
		if f, err := os.Open(chunk.ChunkPath); err == nil {
			scanner := bufio.NewScanner(f)
			for scanner.Scan() {
				line := scanner.Text()
				var r map[string]interface{}; json.Unmarshal([]byte(line), &r)
				cid, _ := r["custom_id"].(string)
				if succeededIDs[cid] || fatalErrorIDs[cid] { continue }
				needRetryLines = append(needRetryLines, line)
			}
			f.Close()
		}
	}

	os.WriteFile(filepath.Join(mergedDir, fmt.Sprintf("output_retry%d.jsonl", retry)), []byte(strings.Join(allOutput, "\n")+"\n"), 0644)
	os.WriteFile(filepath.Join(mergedDir, fmt.Sprintf("missing_records_retry%d.jsonl", retry)), []byte(strings.Join(needRetryLines, "\n")+"\n"), 0644)

	if len(failureReport) > 0 {
		reportPath := filepath.Join(mergedDir, fmt.Sprintf("failure_analysis_retry%d.csv", retry))
		csvFile, _ := os.Create(reportPath); defer csvFile.Close(); csvFile.WriteString("\xEF\xBB\xBF")
		writer := csv.NewWriter(csvFile)
		writer.Write([]string{"CustomID", "çŠ¶æ€", "HTTPçŠ¶æ€ç ", "é”™è¯¯ä»£ç ", "é”™è¯¯ä¿¡æ¯", "å»ºè®®æ“ä½œ"})
		for _, rec := range failureReport {
			status, action := "éœ€é‡è¯•", "ç³»ç»Ÿå°†è‡ªåŠ¨é‡è¯•"
			if rec.IsFatal { status, action = "å·²æ”¾å¼ƒ", "è¯·æ£€æŸ¥Prompté•¿åº¦/æ ¼å¼/æ¨¡å‹" }
			writer.Write([]string{rec.CustomID, status, fmt.Sprintf("%d", rec.StatusCode), rec.ErrorCode, rec.ErrorMsg, action})
		}
		writer.Flush()
	}

	// æœ€ç»ˆåˆå¹¶é€»è¾‘ï¼šå¦‚æœè¾¾åˆ°äº† MaxRetry æˆ–è€… æ²¡æœ‰éœ€è¦é‡è¯•çš„æ•°æ®ï¼Œåˆ™è§†ä¸ºå®Œæˆ
	if retry >= fileInfo.MaxRetry || len(needRetryLines) == 0 {
		finalOut := []string{}
		for r := 0; r <= retry; r++ {
			if d, err := os.ReadFile(filepath.Join(mergedDir, fmt.Sprintf("output_retry%d.jsonl", r))); err == nil {
				finalOut = append(finalOut, string(d))
			}
		}
		os.WriteFile(filepath.Join(mergedDir, "output.jsonl"), []byte(strings.Join(finalOut, "")), 0644)
		fm.dbManager.UpdateFileStatus(taskID, FileStatusProcessCompleted, nil)
	}

	return map[string]interface{}{"missing_count": len(needRetryLines), "fatal_count": len(fatalErrorIDs)}, nil
}

// RetryFailedRecords: ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼éµå®ˆ MaxRetryï¼Œé˜²æ­¢ç”Ÿæˆå¹½çµåˆ†å—
func (fm *FileManager) RetryFailedRecords(taskID string) (bool, error) {
	fileInfo, _ := fm.dbManager.GetFile(taskID)

	// ã€æ–°å¢ã€‘ä¸¥æ ¼æ£€æŸ¥ï¼šå¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç›´æ¥åœæ­¢ï¼Œç»ä¸ç”Ÿæˆæ–°çš„åˆ†å—
	if fileInfo.Retry >= fileInfo.MaxRetry {
		logInfo("ä»»åŠ¡ [%s] å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° (%d)ï¼Œåœæ­¢ç”Ÿæˆé‡è¯•åˆ†å—ã€‚", taskID, fileInfo.MaxRetry)
		// è¿”å› trueï¼Œå‘ŠçŸ¥ä¸Šå±‚ä»»åŠ¡å·²ç»“æŸ
		return true, nil 
	}

	missingPath := filepath.Join(MERGED_DIR, taskID, fmt.Sprintf("missing_records_retry%d.jsonl", fileInfo.Retry))
	data, _ := os.ReadFile(missingPath)
	if len(data) == 0 { return true, nil }

	newRetry := fileInfo.Retry + 1
	fm.dbManager.UpdateFileRetry(taskID, newRetry)
	lines := strings.Split(strings.TrimSpace(string(data)), "\n")
	chunkDir := filepath.Join(CHUNK_DIR, taskID)
	chunkIndex, currentChunkLines := 0, []string{}
	for _, line := range lines {
		currentChunkLines = append(currentChunkLines, line)
		if len(currentChunkLines) >= LINES_PER_CHUNK {
			fm.writeChunk(taskID, chunkIndex, fileInfo.OriginalFilename, chunkDir, currentChunkLines, fileInfo, newRetry)
			chunkIndex++; currentChunkLines = []string{}
		}
	}
	if len(currentChunkLines) > 0 {
		fm.writeChunk(taskID, chunkIndex, fileInfo.OriginalFilename, chunkDir, currentChunkLines, fileInfo, newRetry)
		chunkIndex++
	}
	fm.dbManager.UpdateFileTotalChunks(taskID, fileInfo.TotalChunks+chunkIndex)
	return false, nil
}