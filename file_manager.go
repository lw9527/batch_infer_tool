package main

import (
	"bufio"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"time"

	"github.com/google/uuid"
)

type FileManager struct {
	dbManager *DBManager
}

func NewFileManager(db *DBManager) *FileManager {
	return &FileManager{dbManager: db}
}

func (fm *FileManager) generateTaskID(filename string) string {
	return fmt.Sprintf("task_%s_%s", time.Now().Format("20060102_150405"), uuid.New().String()[:8])
}

func (fm *FileManager) generateChunkID(taskID string, chunkIndex int, retry int) string {
	if retry > 0 {
		return fmt.Sprintf("%s_retry%d_chunk_%d", taskID, retry, chunkIndex)
	}
	return fmt.Sprintf("%s_chunk_%d", taskID, chunkIndex)
}

func (fm *FileManager) writeChunk(taskID string, chunkIndex int, originalFilename string,
	chunkDir string, currentChunkLines []string, fileInfo *FileInfo, retry int) error {
	chunkID := fm.generateChunkID(taskID, chunkIndex, retry)
	var chunkFilename string
	if retry > 0 {
		chunkFilename = fmt.Sprintf("retry%d_part%d.%s", retry, chunkIndex, originalFilename)
	} else {
		chunkFilename = fmt.Sprintf("part%d.%s", chunkIndex, originalFilename)
	}
	chunkPath := filepath.Join(chunkDir, chunkFilename)
	chunkData := strings.Join(currentChunkLines, "\n")
	os.WriteFile(chunkPath, []byte(chunkData), 0644)
	chunk := &FileChunk{
		ChunkID:    chunkID,
		TaskID:     taskID,
		ChunkIndex: chunkIndex,
		ChunkPath:  chunkPath,
		ChunkSize:  len([]byte(chunkData)),
		Status:     ChunkStatusPending,
		Retry:      retry,
	}
	return fm.dbManager.AddChunk(chunk)
}

// SplitFile: åŒ…å«é”™è¯¯æ‹¦æˆªã€è®°å½•ã€æš‚åœå±•ç¤ºç»Ÿè®¡ä»¥åŠã€é¢„ä¼°è€—æ—¶æ—¥å¿—ã€‘åŠŸèƒ½
func (fm *FileManager) SplitFile(filePath string, originalFilename string, linesPerChunk int, taskID string) (*FileInfo, error) {
	file, err := os.Open(filePath)
	if err != nil {
		return nil, fmt.Errorf("æ–‡ä»¶ä¸å­˜åœ¨")
	}
	defer file.Close()

	if taskID == "" {
		taskID = fm.generateTaskID(originalFilename)
	}
	exists, _ := fm.dbManager.CheckTaskIDExists(taskID)
	if exists {
		return nil, fmt.Errorf("task_id [%s] å·²å­˜åœ¨", taskID)
	}

	stat, _ := file.Stat()
	fileInfoObj := &FileInfo{
		TaskID:           taskID,
		OriginalFilename: originalFilename,
		FilePath:         filePath,
		FileSize:         stat.Size(),
		CreatedTime:      time.Now().Format(time.RFC3339),
		UpdatedTime:      time.Now().Format(time.RFC3339),
		Status:           FileStatusSplitting,
		Chunks:           []*FileChunk{},
		MaxRetry:         MAX_RETRY_COUNT,
	}
	fm.dbManager.CreateFile(fileInfoObj)

	chunkDir := filepath.Join(CHUNK_DIR, taskID)
	os.MkdirAll(chunkDir, 0755)

	// --- å‡†å¤‡é”™è¯¯è®°å½•æ–‡ä»¶ ---
	validationDir := filepath.Join(BATCH_RESULT_DIR, taskID, "validation")
	os.MkdirAll(validationDir, 0755)
	errorFilePath := filepath.Join(validationDir, "format_errors.jsonl")

	errorFile, err := os.Create(errorFilePath)
	var errorWriter *bufio.Writer
	if err == nil {
		errorWriter = bufio.NewWriter(errorFile)
		defer errorFile.Close()
		defer errorWriter.Flush()
	}

	scanner := bufio.NewScanner(file)
	buf := make([]byte, 0, 64*1024)
	scanner.Buffer(buf, 10*1024*1024)

	chunkIndex := 0
	totalLines := 0
	scanLineIndex := 0
	errorCount := 0
	currentChunkLines := []string{}

	fmt.Printf("æ­£åœ¨æ‰«ææ–‡ä»¶: %s ...\n", originalFilename)

	for scanner.Scan() {
		scanLineIndex++
		rawLine := scanner.Text()
		line := strings.TrimSpace(rawLine)

		if line == "" {
			continue
		}

		var originJSON map[string]interface{}
		var parseError string = ""

		// 1. JSON è¯­æ³•æ£€æŸ¥
		if err := json.Unmarshal([]byte(line), &originJSON); err != nil {
			parseError = "Invalid JSON syntax"
		} else {
			// 2. ä¸šåŠ¡å­—æ®µæ£€æŸ¥
			if _, ok := originJSON[ModelConf.MessagesKey].([]interface{}); !ok {
				parseError = fmt.Sprintf("Missing or invalid field: %s", ModelConf.MessagesKey)
			}
		}

		// 3. é”™è¯¯å¤„ç†
		if parseError != "" {
			errorCount++
			if errorWriter != nil {
				errRecord := map[string]interface{}{
					"line_no": scanLineIndex,
					"error":   parseError,
					"content": line,
				}
				if errBytes, err := json.Marshal(errRecord); err == nil {
					errorWriter.WriteString(string(errBytes) + "\n")
				}
			}
			if errorCount <= 5 {
				fmt.Printf("âš ï¸  [ç¬¬ %d è¡Œ] æ ¼å¼é”™è¯¯: %s\n", scanLineIndex, parseError)
			}
			continue
		}

		// 4. æœ‰æ•ˆæ•°æ®å¤„ç†
		messages, _ := originJSON[ModelConf.MessagesKey].([]interface{})
		body := map[string]interface{}{"model": ModelConf.Domain, "messages": messages, "max_tokens": ModelConf.MaxTokens}
		if ModelConf.Temperature != nil {
			body["temperature"] = *ModelConf.Temperature
		}
		if ModelConf.TopP != nil {
			body["top_p"] = *ModelConf.TopP
		}

		newline := map[string]interface{}{
			"custom_id": fmt.Sprintf("%d", totalLines),
			"method":    "POST", "url": "/v1/chat/completions", "body": body,
		}
		newlineJSON, _ := json.Marshal(newline)
		currentChunkLines = append(currentChunkLines, string(newlineJSON))
		totalLines++

		if TEST_LINES > 0 && totalLines >= TEST_LINES {
			break
		}

		if len(currentChunkLines) >= linesPerChunk {
			fm.writeChunk(taskID, chunkIndex, originalFilename, chunkDir, currentChunkLines, fileInfoObj, fileInfoObj.Retry)
			chunkIndex++
			currentChunkLines = []string{}
		}
	}

	if len(currentChunkLines) > 0 {
		fm.writeChunk(taskID, chunkIndex, originalFilename, chunkDir, currentChunkLines, fileInfoObj, fileInfoObj.Retry)
		chunkIndex++
	}

	if errorWriter != nil {
		errorWriter.Flush()
	}

	// --- 5. æ‰“å°ç»Ÿè®¡å¹¶è°ƒç”¨æ—¥å¿—è®°å½•æ—¶é—´ ---
	fmt.Println("----------------------------------------------------------------")
	
	// è®¡ç®—å¹¶è®°å½•é¢„ä¼°æ—¶é—´ (è°ƒç”¨ logger.go ä¸­çš„æ–°æ–¹æ³•)
	// è¿™ä¼šå°† "â³ [é¢„ä¼°è€—æ—¶]..." å†™å…¥ app.log å¹¶æ˜¾ç¤ºåœ¨ç»ˆç«¯
	taskLogger := NewTaskLogger(taskID)
	taskLogger.LogTimeEstimate(totalLines) 

	fmt.Printf("ğŸ“Š æ–‡ä»¶æ‰«æç»Ÿè®¡ | ä»»åŠ¡ID: %s\n", taskID)
	fmt.Printf("   - æ‰«ææ€»è¡Œæ•°: %d\n", scanLineIndex)
	fmt.Printf("   - âœ… æœ‰æ•ˆæ•°æ®: %d\n", totalLines)
	fmt.Printf("   - âŒ é”™è¯¯æ•°æ®: %d\n", errorCount)

	if errorCount > 0 {
		absPath, _ := filepath.Abs(errorFilePath)
		fmt.Printf("\nğŸ“‚ é”™è¯¯è¡Œå·²å•ç‹¬ä¿å­˜è‡³:\n   %s\n", absPath)
		fmt.Println("\nâš ï¸  æ£€æµ‹åˆ°æ ¼å¼é”™è¯¯ï¼Œè¯·é˜…è¯»ä»¥ä¸Šç»Ÿè®¡ä¿¡æ¯ (ç³»ç»Ÿå°†åœ¨ 5ç§’ åè‡ªåŠ¨ç»§ç»­)...")
		time.Sleep(5 * time.Second)
	} else {
		fmt.Println("\nâœ… æ ¡éªŒé€šè¿‡ï¼Œå‡†å¤‡å¼€å§‹å¤„ç†ä»»åŠ¡ (2ç§’åå¼€å§‹)...")
		time.Sleep(2 * time.Second)
	}
	fmt.Println("----------------------------------------------------------------")

	fileInfoObj.TotalChunks = chunkIndex
	fileInfoObj.TotalLines = totalLines
	fm.dbManager.UpdateFileStatus(taskID, FileStatusSplitCompleted, nil)
	fm.dbManager.UpdateFileTotalChunks(taskID, chunkIndex)
	fm.dbManager.UpdateFileTotalLines(taskID, totalLines)
	return fileInfoObj, nil
}

func (fm *FileManager) SaveFile(taskID string, chunkID string, fileContent string, isError bool) error {
	chunk, _ := fm.dbManager.GetChunk(chunkID)
	if chunk == nil {
		return fmt.Errorf("chunk not found")
	}
	path := filepath.Join(BATCH_RESULT_DIR, taskID, "output")
	if isError {
		path = filepath.Join(BATCH_RESULT_DIR, taskID, "error")
	}
	os.MkdirAll(path, 0755)
	return os.WriteFile(filepath.Join(path, fmt.Sprintf("retry%d_%s.jsonl", chunk.Retry, chunkID)), []byte(fileContent), 0644)
}

// isFatalError: åˆ¤æ–­æ˜¯å¦ä¸ºä¸å¯é‡è¯•çš„è‡´å‘½é”™è¯¯
func (fm *FileManager) isFatalError(statusCode int, errCode string) (bool, string) {
	// 1. çŠ¶æ€ç åˆ¤æ–­: 400-499 é€šå¸¸æ˜¯è¯·æ±‚é—®é¢˜ (é™¤äº† 429 Rate Limit)
	if statusCode >= 400 && statusCode < 500 {
		if statusCode == 429 {
			return false, "Rate Limit (Retryable)"
		}
		return true, fmt.Sprintf("HTTP %d (Fatal)", statusCode)
	}

	// 2. é”™è¯¯ç åˆ¤æ–­ (OpenAI/DeepSeek/Spark æ ‡å‡†é”™è¯¯ç )
	fatalCodes := map[string]bool{
		"context_length_exceeded":  true, // ä¸Šä¸‹æ–‡è¶…é•¿
		"invalid_request_error":    true, // è¯·æ±‚æ ¼å¼é”™è¯¯
		"invalid_api_key":          true, // Key é”™è¯¯
		"unknown_url":              true, // URL é”™è¯¯
		"string_above_128k_tokens": true, // å…·ä½“æ¨¡å‹é™åˆ¶
		"model_not_found":          true, // æ¨¡å‹åé”™è¯¯
		"10003":                    true, // Spark: invalid role / å‚æ•°é”™è¯¯
		"invalid_role":             true, // è§’è‰²é”™è¯¯
		"INVALID_PAYLOAD":          true, // è‡ªå®šä¹‰ï¼špayload é”™è¯¯
	}

	if fatalCodes[errCode] {
		return true, fmt.Sprintf("Error Code: %s", errCode)
	}

	return false, "Retryable Error"
}

// MergeBatchResults: åŒ…å«å¯¹â€œå‡æˆåŠŸâ€æ•°æ®ï¼ˆHTTP 101 ä½† code!=0ï¼‰çš„è¯†åˆ«
// ä»¥åŠ CSV æŠ¥å‘Šç”Ÿæˆ
func (fm *FileManager) MergeBatchResults(taskID string, retry int) (map[string]interface{}, error) {
	fileInfo, err := fm.dbManager.GetFile(taskID)
	if err != nil || fileInfo == nil {
		return nil, fmt.Errorf("ä»»åŠ¡ä¸å­˜åœ¨")
	}

	var chunks []*FileChunk
	for _, c := range fileInfo.Chunks {
		if c.Retry == retry {
			chunks = append(chunks, c)
		}
	}
	sort.Slice(chunks, func(i, j int) bool { return chunks[i].ChunkIndex < chunks[j].ChunkIndex })

	allOutput := []string{}
	succeededIDs := make(map[string]bool)
	fatalErrorIDs := make(map[string]bool)

	type FailureRecord struct {
		CustomID   string
		StatusCode int
		ErrorCode  string
		ErrorMsg   string
		IsFatal    bool
	}
	failureReport := []FailureRecord{}

	mergedDir := filepath.Join(MERGED_DIR, taskID)
	os.MkdirAll(mergedDir, 0755)

	// --- 1. è¯»å– Output å’Œ Error æ–‡ä»¶ ---
	for _, chunk := range chunks {
		// A. Output (æˆåŠŸè®°å½•)
		outputFile := filepath.Join(BATCH_RESULT_DIR, taskID, "output", fmt.Sprintf("retry%d_%s.jsonl", chunk.Retry, chunk.ChunkID))
		if f, err := os.Open(outputFile); err == nil {
			scanner := bufio.NewScanner(f)
			for scanner.Scan() {
				line := scanner.Text()
				allOutput = append(allOutput, line)
				var r map[string]interface{}
				if json.Unmarshal([]byte(line), &r) == nil {
					if cid, ok := r["custom_id"].(string); ok {
						succeededIDs[cid] = true
					}
				}
			}
			f.Close()
		}

		// B. Error (å¤±è´¥è®°å½•)
		errorFile := filepath.Join(BATCH_RESULT_DIR, taskID, "error", fmt.Sprintf("retry%d_%s.jsonl", chunk.Retry, chunk.ChunkID))
		if f, err := os.Open(errorFile); err == nil {
			scanner := bufio.NewScanner(f)
			for scanner.Scan() {
				line := scanner.Text()
				
				// å®šä¹‰è§£æç»“æ„ (æ”¯æŒ Spark ç­‰è¿”å›çš„ code ä¸º int çš„æƒ…å†µ)
				var r struct {
					CustomID string `json:"custom_id"`
					Response struct {
						StatusCode int `json:"status_code"`
						Body       struct {
							Code    interface{} `json:"code"` // æ”¯æŒæ•°å­—æˆ–å­—ç¬¦ä¸²
							Message string      `json:"message"`
							Error   struct {
								Code    string `json:"code"`
								Message string `json:"message"`
							} `json:"error"`
						} `json:"body"`
					} `json:"response"`
				}

				if json.Unmarshal([]byte(line), &r) == nil && r.CustomID != "" {
					
					// ================== æ ¸å¿ƒé€»è¾‘ä¿®æ­£ ==================
					
					// 1. æå–ä¸šåŠ¡çŠ¶æ€ç  (Business Code)
					var bizCode int = 0
					if r.Response.Body.Code != nil {
						switch v := r.Response.Body.Code.(type) {
						case float64:
							bizCode = int(v)
						case int:
							bizCode = v
						}
					}

					// 2. åˆ¤æ–­æ˜¯å¦çœŸçš„æ˜¯æˆåŠŸ
					// å¦‚æœ HTTP < 300 ä¸” ä¸šåŠ¡Code == 0ï¼Œæ‰æ˜¯çœŸæˆåŠŸï¼Œå¿½ç•¥å®ƒ
					if r.Response.StatusCode > 0 && r.Response.StatusCode < 300 && bizCode == 0 {
						continue
					}

					// 3. æå–é”™è¯¯ä¿¡æ¯
					finalErrorCode := r.Response.Body.Error.Code
					finalErrorMsg := r.Response.Body.Error.Message
					
					// å…¼å®¹ Spark é”™è¯¯æ ¼å¼ (body.code != 0 ä½† error å­—æ®µä¸ºç©ºçš„æƒ…å†µ)
					if finalErrorCode == "" && bizCode != 0 {
						finalErrorCode = fmt.Sprintf("%d", bizCode)
						finalErrorMsg = r.Response.Body.Message
					}

					// Case 2: éæ ‡å‡†é”™è¯¯ (StatusCode == 0ï¼Œæ—  Response å­—æ®µ)
					if r.Response.StatusCode == 0 {
						fatalErrorIDs[r.CustomID] = true
						failureReport = append(failureReport, FailureRecord{
							CustomID:   r.CustomID,
							StatusCode: 400,
							ErrorCode:  "INVALID_PAYLOAD",
							ErrorMsg:   "APIç›´æ¥æ‹’æ”¶è¯·æ±‚ï¼ŒPayloadæ ¼å¼ä¸¥é‡é”™è¯¯",
							IsFatal:    true,
						})
						continue
					}

					// Case 3: çœŸæ­£çš„é”™è¯¯
					isFatal, _ := fm.isFatalError(r.Response.StatusCode, finalErrorCode)
					if isFatal {
						fatalErrorIDs[r.CustomID] = true
					}

					failureReport = append(failureReport, FailureRecord{
						CustomID:   r.CustomID,
						StatusCode: r.Response.StatusCode,
						ErrorCode:  finalErrorCode,
						ErrorMsg:   finalErrorMsg,
						IsFatal:    isFatal,
					})
				}
			}
			f.Close()
		}
	}

	// --- 2. ç­›é€‰éœ€è¦é‡è¯•çš„æ•°æ® ---
	needRetryLines := []string{}
	for _, chunk := range chunks {
		chunkInputFile := chunk.ChunkPath
		if f, err := os.Open(chunkInputFile); err == nil {
			scanner := bufio.NewScanner(f)
			for scanner.Scan() {
				line := scanner.Text()
				var r map[string]interface{}
				json.Unmarshal([]byte(line), &r)
				cid, _ := r["custom_id"].(string)

				if succeededIDs[cid] { continue }
				if fatalErrorIDs[cid] { continue } // è‡´å‘½é”™è¯¯ä¸é‡è¯•
				
				needRetryLines = append(needRetryLines, line)
			}
			f.Close()
		}
	}

	// --- 3. å†™å…¥æ–‡ä»¶ ---
	os.WriteFile(filepath.Join(mergedDir, fmt.Sprintf("output_retry%d.jsonl", retry)), []byte(strings.Join(allOutput, "\n")+"\n"), 0644)
	os.WriteFile(filepath.Join(mergedDir, fmt.Sprintf("missing_records_retry%d.jsonl", retry)), []byte(strings.Join(needRetryLines, "\n")+"\n"), 0644)

	// ç”Ÿæˆ CSV å¤±è´¥åˆ†ææŠ¥å‘Š
	if len(failureReport) > 0 {
		reportPath := filepath.Join(mergedDir, fmt.Sprintf("failure_analysis_retry%d.csv", retry))
		csvFile, _ := os.Create(reportPath)
		defer csvFile.Close()
		csvFile.WriteString("\xEF\xBB\xBF") 
		writer := csv.NewWriter(csvFile)
		writer.Write([]string{"CustomID", "çŠ¶æ€", "HTTPçŠ¶æ€ç ", "é”™è¯¯ä»£ç ", "é”™è¯¯ä¿¡æ¯", "å»ºè®®æ“ä½œ"})
		for _, rec := range failureReport {
			status := "éœ€é‡è¯•"
			action := "ç³»ç»Ÿå°†è‡ªåŠ¨é‡è¯•"
			if rec.IsFatal {
				status = "å·²æ”¾å¼ƒ"
				action = "è¯·æ£€æŸ¥Prompté•¿åº¦/æ ¼å¼/æ¨¡å‹"
			}
			writer.Write([]string{
				rec.CustomID, status, fmt.Sprintf("%d", rec.StatusCode),
				rec.ErrorCode, rec.ErrorMsg, action,
			})
		}
		writer.Flush()
		fmt.Printf("\nğŸ“‹ å·²ç”Ÿæˆå¤±è´¥åˆ†ææŠ¥å‘Š: %s\n", reportPath)
	}

	// --- 4. æœ€ç»ˆåˆå¹¶ ---
	if retry == fileInfo.MaxRetry || len(needRetryLines) == 0 {
		finalOut := []string{}
		for r := 0; r <= retry; r++ {
			if d, err := os.ReadFile(filepath.Join(mergedDir, fmt.Sprintf("output_retry%d.jsonl", r))); err == nil {
				finalOut = append(finalOut, string(d))
			}
		}
		os.WriteFile(filepath.Join(mergedDir, "output.jsonl"), []byte(strings.Join(finalOut, "")), 0644)
		fm.dbManager.UpdateFileStatus(taskID, FileStatusProcessCompleted, nil)
	}

	return map[string]interface{}{
		"missing_count": len(needRetryLines),
		"fatal_count":   len(fatalErrorIDs),
	}, nil
}

func (fm *FileManager) RetryFailedRecords(taskID string) (bool, error) {
	fileInfo, _ := fm.dbManager.GetFile(taskID)
	missingPath := filepath.Join(MERGED_DIR, taskID, fmt.Sprintf("missing_records_retry%d.jsonl", fileInfo.Retry))
	data, _ := os.ReadFile(missingPath)
	if len(data) == 0 {
		return true, nil
	}

	newRetry := fileInfo.Retry + 1
	fm.dbManager.UpdateFileRetry(taskID, newRetry)
	lines := strings.Split(strings.TrimSpace(string(data)), "\n")
	chunkDir := filepath.Join(CHUNK_DIR, taskID)
	chunkIndex, currentChunkLines := 0, []string{}

	for _, line := range lines {
		currentChunkLines = append(currentChunkLines, line)
		if len(currentChunkLines) >= LINES_PER_CHUNK {
			fm.writeChunk(taskID, chunkIndex, fileInfo.OriginalFilename, chunkDir, currentChunkLines, fileInfo, newRetry)
			chunkIndex++
			currentChunkLines = []string{}
		}
	}
	if len(currentChunkLines) > 0 {
		fm.writeChunk(taskID, chunkIndex, fileInfo.OriginalFilename, chunkDir, currentChunkLines, fileInfo, newRetry)
		chunkIndex++
	}
	fm.dbManager.UpdateFileTotalChunks(taskID, fileInfo.TotalChunks+chunkIndex)
	return false, nil
}